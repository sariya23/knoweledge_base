[[Hive]]
![[Pasted image 20250628104631.png]]
Представим ситуацию, что мы хотим взять данные с HDFS, именно данные, а не файлы! Есть 2 ситуации
### Хотим прочитать данные с HDFS через Spark
1. Пишем код на PySpark
2. Spark идет с этим запросом в Hive и требует данные с HDFS
3. Hive переводит код Spark на HiveQL
4. HiveQL автоматически оптимизируется
5. Далее создается план запроса
6. Запрос HiveQL переводится в MapReduce задачи, которые выполняет уже Hadoop
### Чтение данные бел клиента
Все тоже самое, например, пишем HiveQL через CLI. В этой случае просто нет спарка)
## Компоненты Hive
![[Pasted image 20250628110116.png]]

- CLI - через CLI можем писать 
- WEB - им почти не пользуются. Он для выполнения запросов и управления задачами
- JDBC, ODBC - интерфейсы, которые позволяют подключаться к Hive через стандартные драйверы
- Thrift Server - приложения, которые не поддерживают JDBC или ODBC, подключатся через Thrift Server
- Metastore - хранилище метаданных, которое содержит информацию о хранящихся таблицах в Hive
- Driver - центральный компонент, который обрабатывает запросы. Он выполняет 3 ключевые функции: 
	- Компилятор - преобразует HiveQL в MapReduce задачи
	- Оптимизатор 
	- Исполнитель