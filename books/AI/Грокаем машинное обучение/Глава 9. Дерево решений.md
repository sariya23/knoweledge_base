![[Pasted image 20250405152159.png]]
Модель деревьев решения прогнозирует результат, основываясь на условиях да/нет. Когда алгоритм доходит до листа, выдает прогноз.
## Дерево решений для рекомендации
Есть три приложения - Счетчик атомов, Карта ульев и Австралотомат. И такой вот набор данных
![[Pasted image 20250405152423.png]]
Вкратце алгоритм построения дерева выглядит так:
- Выяснить какие данные наиболее полезны для решения о том, какое приложение рекомендовать
- Определить признак, при котором данные разбиваются на 2 меньшие группы
- Повторить 1 и 2 для каждого из двух меньших наборов
### Выбор первого вопроса
Первым делом нам надо определить самый полезный признак. То есть вопрос "Молодой ли пользователь" и "У пользователя iPhone"
#### У пользователя iPhone?
Если мы выберем этот вопрос, то получится вот такое дерево
![[Pasted image 20250405152745.png]]
#### Молодой ли пользователь?
Если выберем этот вопрос, то получим вот такое дерево
![[Pasted image 20250405152830.png]]
### Выбор наилучшего вопроса
А как компу понять, какой из этих вопросов лучше? Для этого есть 3 способа сравнения:
- достоверность
- расхождение Джини
- энтропия 
#### Достоверность
Достоверность это та же метрика, что и в модели классификации - отношение кол-ва правильных прогнозам к общему числу прогнозов.
При классификации по платформе достоверность - 4/6
При классификации по возрасту - 5/6
Следовательно, лучше использовать классификатор по возрасту
### Индекс примесей Джини
Этот индекс показывает, насколько набор данных разнообразный. Чем он ниже, тем менее разнообразный набор данных. Если он 0 - то все элементы в наборе одинаковые
**Индекс примесей Джини**: в наборе с $m$ элементами, $n$ классами и $a_i$ элементами, принадлежащими к $i$-му классу 
$$
индекс=1-p_1^2-p_2^2-...-p_n^2
$$
Где $p_i=a_i/m$
Посчитаем индекс Джини для разделения по платформе:
$$
G_{iphone}=1-(\frac{2}{3})^2-(\frac{1}{3})^2=0.444
$$
$$
G_{!iphone}=1-(\frac{2}{3})^2-(\frac{1}{3})^2=0.444
$$
Тут у нас 2 класса - платформа iPhone и не iPhone. Элементов в каждом классе по 3($m$), а элементы - это разные приложения. Первый набор ${\{С, А, А\}}$ - соответсвенно вероятность вытащить $C$ - $\frac{1}{3}$, а $A$ - $\frac{2}{3}$.   У второго набора индекс такой же. 
А теперь посчитаем его для разделения по возрасту
$$
G_{young}=1-(\frac{3}{3})^2=0
$$
$$
G_{old}=1-(\frac{2}{3})^2-(\frac{1}{3})^2=0.444
$$
Для измерения чистоты разделения просто усредняем его
![[Pasted image 20250405154741.png]]
При классификации по возрасту индекс ниже, значит тут разделение лучше
### Энтропия
Энтропия тоже показывает разнообразие данных. 
**Энтропия**: в наборе с $m$ элементами, $n$ классами и $a_i$ элементами, принадлежащим к итому классу:
$$
a=-p_1log_2(p1)-p_2log_2(p2)-...-p_nlog_2(p_n)
$$
Где $p_i=a_i/m$
Посчитаем энтропию для классификатора по платформе
$$
a_{iphone}=-\frac{2}{3}*log_2(\frac{2}{3})-\frac{1}{3}*log_2(\frac{1}{3})=0.918
$$
$$
a_{!iphone}=-\frac{3}{3}*log_2(\frac{3}{3})=0
$$
![[Pasted image 20250405155625.png]]
Для каждого набора надо высчитывать энтропию, тут просто они одинаковые, поэтому я не стал дублировать.
### Средневзвешенное значение
У нас может быть ситуация, что классы разных размеров. Для этого нужно помножить индекс Джини класса на отношение размера класса к общему кол-ву элементов
![[Pasted image 20250405155759.png]]
### Когда прекращать?
Чтобы не сделать модель переобученной, нужно понять, когда остановится. Это могут быть такие гиперпараметры:
- Минимальная величина изменения достоверности(индекса Джини или энтропии)
- Минимальное кол-во значений в узле, чтобы его можно было разделить
- Минимальное кол-во элементов в листе
- Максимальная глубина дерева
### Разделение данных с помощью непрерывных признаков
Мы должны задавать вопросы, на которые можно ответь Да или Нет. В случае с возрастом у нас может быть бесконечно много таких вопросов. В таком случае нам нужно выяснить, какое пороговое значение возраста показывает себя лучше всего (по индексу Джини, энтропии или достоверности)