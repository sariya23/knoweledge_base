Алгоритм классифицировал метки как 1 или 0. Модель логистической регрессии позволяет классифицировать метки вероятностью. Для этого используются функции сигмоиды
$$
\sigma(x)=\frac{1}{1+e^{-x}}
$$
Эта функция сжимает строку действительных чисел в промежуток от 0 до 1. Ступенчатая функция выдает 0 для любого отрицательного входного сигнала, а сигмоида выдает значение меньше 0.5
![[Pasted image 20250403161917.png]]
Сигмоидная лучше дискретной тем, что она дает больше информации. Мы можем знать вероятность того, что метка является тем, кем является.
Вот есть такие иксы и результаты сигмоиды

| $x$ | $\sigma(x)$ |
| --- | ----------- |
| -5  | 0.007       |
| -1  | 0.269       |
| 0   | 0.5         |
| 1   | 0.731       |
| 5   | 0.993       |
## Набор данных
Использоваться будет такой набор данных
![[Pasted image 20250403162156.png]]
Уравнение прямой то же, что и в 5 главе: $x_{аак}+2x_{бип}-4$. Но теперь мы будет использовать не ступенчатую функцию, а сигмоиду: $\hat{y}=\sigma(x_{аак}+2x_{бип}-4)$. У нас получаются следующие прогнозы
1. $\hat{y}=\sigma(3 + 2*2 - 4)=\sigma(3)=0.953$
2. $\hat{y}=\sigma(1+2*2-4)=\sigma(1)=0.731$
3. $\hat{y}=\sigma(0+2*1-4)=\sigma(-2)=0.119$
4. $\hat{y}=\sigma(2+2*0-4)=\sigma(-2)=0.119$
Вот так это выглядит на графике
![[Pasted image 20250403162711.png]]
## Функция ошибок
Абсолютные и квадратичные ошибки не очень подойдут, так как числа тут очень маленькие, то их сумма тоже будет не очень велика. В общем тут нужно кое-что другое
### Логистическая функция потерь
Тут уже не "ошибка", а "потеря". А логистическая, потому что в формуле будет логарифм. Результат сигмоиды можно считать вероятностью. Вероятность эта равна тому, что метка "радостная". А вероятность того, что грустая - $1-вероястноть$. Цель модели в том, чтобы присвоить высокую вероятность радостным точкам и низку грустным. 
**Точка 1**:
- метка - 0(грустный)
- прогноз - 0.953 (вероятность того, что точка радостная, далее так же)
- вероятность того, что прогноз равен метки - $1-0.953=0.047$
**Точка 2**:
- метка - 1(радостный)
- прогноз 0.731
- вероятность того, что прогноз равен метки 0.731
**Точка 3**
- метка - 1(радостный)
- прогноз 0.119
- вероятность того, что прогноз равен метки 0.119
**Точка 4**:
- метка - 0(грустный)
- прогноз 0.119
- Вероятность того, что прогноз равен метки $1-0.119=0.881$
Точки 2 и 4 хорошо классифицированы, поэтому вероятности у них высокие. А вот точки 1 и 3 дают низку вероятность.
И как получить функции ошибки? Перемножить не вариант, так как по итогу получится очень маленькое число, которое комп даже не обработает. Надо как-то превратить в сумму. Но это вероятности и их нужно перемножать (в данном случае). Превратить произведение в сумму можем логарифм
$$
ln(ab)=ln(a)+ln(b)
$$
По итогу у нас получится
$$
ln(0.047*0.731*0.119*0.881)=ln(0.047)+ln(0.731)+ln(0.119)+ln(0.881)=-5.616
$$
Число получается отрицательным, так как мы берем натуральный логарифм от числа о 0 до 1. Поэтому можем просто домножить на -1
![[Pasted image 20250403163841.png]]
Как видно у хорошо классифицированных точек потеря мала.
### Алгоритм для логистической потери
- Если метка равно 0, то потеря равна $-ln(1-\hat{y})$
- Если метка равна 1, то потеря равна $-ln(\hat{y})$
Все это можно свести к одной формуле:
$$
loss=-yln(\hat{y})-(1-y)ln(1-\hat{y})
$$
Если метка 0(грустный), то первая часть занулиться и останется только $loss=-ln(1-\hat{y})$. Если же 1, то занулится вторая часть и останется только $-ln(\hat{y})$
Теперь применим эту ошибку для сравнения двух классификаторов
![[Pasted image 20250403164246.png]]
Посчитаем для каждого из них функцию потери.
**Классификатор 1**
1. $loss=-ln(1-0.953)=3.049$
2. $loss=-ln(0.731)=0.313$
3. $loss=-ln(0.119)=2.127$
4. $loss=-ln(1-0.881)=2.12$ (у автора ошибка, в 4 прогноз должен быть 0.119)
**Total loss**: $7.609$
**Классификатор 2**
5. $loss=-ln(1-0.269)=0.313$
6. $loss=-ln(0.731)=0.313$
7. $loss=-ln(0.731)=0.313$
8. $loss=-ln(1-0.119)=0.126$
**Total losss:** $1.065$
Получается, что второй классификатор лучше
## Алгоритм логистического метода
Алгоритм похож на алгоритм перспетрона. Отличие в том, что если точка находится в правильной зоне, мы будет линию отодвигать от нее.
- Если точка классифицирована правильно, то слегка отодвинуть от нее прямую
- Если точка классифицирована неправильно, то слегка пододвинуть прямую к ней
Размер смещения зависит он того, насколько сильно вероятность отличается от факта
![[Pasted image 20250403170033.png]]
А уменьшать или увеличивать зависит от разности метки и прогноза. То есть если прогноз больше метки, значит нам нужно "уменьшить" вероятность. 
По итогу получаем такие формулы
$$
a=a+\eta(y-\hat{y})x_1
$$$$
b=b+\eta(y-\hat{y})x_2
$$$$
c=c+\eta(y-\hat{y})
$$
Мы также как и в случае перспетрона должны помножать на вес
**Алгоритм логистического метода**
**Входные данные**:
- Логистический классификатор с весами $a$, $b$ и bias $c$
- Точка $(x_{1,}x_2)$ и метка $y$
- Скорость обучения $\eta$
**Результат**: новые веса, который уменьшит потерю
**Процедура**: прогноз персептрона по формуле $\hat{y}=\sigma(ax_1+bx_2+c)$
**Возврат**: новые веса $a=a+\eta(y-\hat{y})x_1$, $b=b+\eta(y-\hat{y})x_2$, $c=c+\eta(y-\hat{y})$
## Алгоритм логистической регрессии
**Входные данные**:
- набор точек с метками 1 или 0
- кол-во эпох
- скорость обучения
**Результат**: логистический классификатор
**Процедура**:
- Назначить случайные значения весам и смещению
- Повторять многократно:
	- выбрать случайную точку данных
	- обновить веса и смещение, использую логистический метод
**Возврат**: классифкатор

## Классификация по нескольким классам
А если у нас 3 класса: кошка, собака и птица. Хочется получать вероятность, что, например, это кошка с 10%, собака 90% и птица 5%.
Для этого обучаем 3 классификатора для каждого класса. Например, они вернули
- собака 3
- кошка 2
- птица -1
Теперь надо превратить это в вероятность. Воспользуемся экспонентой
- собака $e^3=20,085$
- кошка $e^2=7.389$
- птица $e^{-1}=0.3$
Посчитаем сумму, получится 27,842 и подели каждое из чисел на эту сумму. Таким образом получим вероятность. Общем случае это запишется так
$$
p_i=\frac{e^{a_i}}{e^{a_1}+e^{a_2}+...+e^{a_n}}
$$
