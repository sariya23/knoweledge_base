[[Грокаем машинное обучение]]
У нас есть задача - спрогнозировать цену дома. У нас есть кол-во квартир в доме и цена за дом. Мы можем изобразить это множество домов на графике
![[Pasted image 20250330151610.png]]
Наша цель в линейной регрессии провести такую линию, чтобы она была к этим точкам максимально близко
![[Pasted image 20250330151653.png]]
В регрессионной модели мы просто выявляем формулу для этой прямой
![[Pasted image 20250330151723.png]]
Например тут, видно, что итоговая цена дома - это 100 + кол-во_комнат * 50. Вот и получили модель линейной регрессии
## Многомерная линейная регрессия
У нас может быть больше переменных на прогнозирования цены за дом. 
```
цена = 30 × количество комнат + 1,5 × размер школы + + 10 × уровень школы – 2 × возраст дома + 50.
```
Тут есть и отрицательные и положительные коэффициенты. Такие хар-ки как кол-во комнат, размер школы *положительно* коррелируют с итоговой ценой. Возраст дома отрицательный, так как чем он старше, тем меньше итоговая цена. Если же признак равен нулю, то это значит, что он не оказывает влияния.
## Алгоритм линейной регрессии
![[Pasted image 20250330152238.png]]
Наклон еще называют slope, а y-пересечение intercept
### Простой подход
В этом подходе мы каждый раз немного перемещаем прямую к точке, чтобы сблизить их. Тут важную роль играем положение точки относительно прямой
![[Pasted image 20250330152425.png]]
Алгоритм простого подхода:
- Входные данные:
	-  линия с наклоном `m`, y-пересечением `b` и уравнением `p=b+mx`
	-  точка с координатами (r, p)
- Результат: линия `p=b+mx`, которая ближе к точке
- Для начала выбираем 2 маленьких случайных числа `n1` и `n2`
- Сценарий 1: 
	- Добавляем `n1` к наклону `m`, `m=m+n1`
	- Добавляем `n2` к пересечению `b`, `b=b+n2`
- Сценарий 2:
	-  Вычитаем `n1` из наклона `m`, `m=m-n1`
	-  Добавляем `n2` к пересечению `b`, `b=b+n2`
- Сценарий 3:
	-  Вычитаем `n1` из наклона `m`, `m=m-n1`
	-  Вычитаем `n2` из пересечения `b`, `b=b-n2`
- Сценарий 4:
	- Добавляем `n1` к наклону `m`, `m=m+n1`
	- Вычитаем `n2` из пересечения `b`, `b=b-n2`
- Возврат: прямая `p=b+mx`
Так как кол-во комнат может быть только положительным, то нам важны только сценарии 1 и 3
### Квадратический подход
Простой  подход основывается на положении точки относительно прямой. Тут мы уже смотрим на расстояние между положением точки и прямой. Мы поднимем линию вверх, если это расстояние (разность цены и прогнозируемой ценой), положительное, иначе опустим. 
Также тут есть термин скорость обучения:
> Скорость обучения - это маленькое число, которое мы выбираем перед обучением модели. Оно помогает убедиться, что модель при обучении меняется по чуть-чуть

С наклоном все примерно так же.
Получается такое алгоритм
- Входные данные:
	- прямая с наклоном `m`, y-пересечением `b` и уравнением `y=b+mx`
	- точка с координатами `(r, p)`
	- Скорость обучения `n`
- Результат: прямая `y=b+mx`, которая находится ближе к точке
- Добавляем к наклону `m` значение `nr(p-pred_p)`. Происходит небольшое вращение прямой по направлению к точке `(p, r)`
- Добавляем к y-пересечению значение `n(p-pred_p)`. Происходит небольшое перемещение к точке `(p, r)`
- Возврат: прямая `y=mx+b`
## Многократное повторение подхода
Если мы будем очень много повторять какой-то из подходов, то в конечном итоге прямая встанет максимально близко ко всем точкам. Каждая итерация называется эпохой или периодом.
![[Pasted image 20250330155138.png]]
Как видно, при первой эпохе линия была очень далеко. А на периодах 51+ уже ближе
## Функция ошибок
Функция ошибок сообщает как плохо или хорошо работает модель
![[Pasted image 20250330155434.png]]
Также функцию ошибок называют еще функция потерь и функция затрат
### Функция ошибок Абсолютная погрешность
Эта функция высчитывается как сумма модулей разницы между фактическим значением метки и предсказанным
![[Pasted image 20250330155612.png]]
### Функция ошибок Квадратичная погрешность
Она определяется как сумма квадратов расстояний между фактическим значением метки и предсказанным
![[Pasted image 20250330155657.png]]
### Усреднение ошибки
На практике чаще используется среднее значение функций погрешности. То есть нужно поделить высчитанное значение на кол-во меток. 
Популярная функция ошибок - RMSE - корень из средней квадратической погрешности. Почему корень? Если бы предсказывали цену дома и ориентировались не на корень, то не смогли бы сказать, на сколько долларов косячит модель. А если мы возьмем корень, то сможем сказать, что модель косячит на `n` долларов.
## Градиентный спуск
Через градиентный спуск мы обучаем линейную регрессию. Градиентный спуск - это поиск наименьшего значения функции. В данном случае мы хотим найти минимальное значение для функции ошибки MSE. 
## Построение графика функции ошибок
Мы можем строить график ошибок RMSE, чтобы посмотреть, как она меняется с количеством эпох и выяснить оптимальное значение
## Полиномиальная регрессия
Иногда данные идут не по линии
![[Pasted image 20250331113451.png]]
## Параметры и гиперпараметры
Любая величина, которую мы устанавливаем перед обучением(скорость обучения, кол-во эпох) - гиперпараметры
Любая величина, которую модель изменяет или вычисляет в процессе обучения - параметр
