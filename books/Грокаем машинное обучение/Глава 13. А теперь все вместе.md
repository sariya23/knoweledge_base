Промашинлерним набор титаника. Проверим какие из пройденных моделей лучше всего справятся с прогнозом
## Предварительная обработка
Сырые данные
![[Pasted image 20250407133302.png]]
### Разбираемся с пропусками
```python
raw_data.isna().sum()
Survived      0
Pclass        0
Name          0
Sex           0
Age         177
SibSp         0
Parch         0
Ticket        0
Fare          0
Cabin       687
Embarked      2
dtype: int64
```
Видим, что в колонке Cabin очень много пропусков. При том условии, что у нас всего 800 данных, то целесообразно этот столбец просто удалить
```python
clean_data = raw_data.drop("Cabin", axis=1)
```
А вот с возрастом интереснее. Возраст важная метка, мы не можем ее просто так удалить, да и данных пропущено относительно мало. Поэтому пропуски медианным значением
```python
median_age = int(clean_data["Age"].median())
clean_data["Age"] = clean_data["Age"].fillna(median_age)
```
Остался столбец Embarked. Там всего 2 пропуска, поэтому можно поставить ему значение `U`
```python
clean_data["Embarked"] = clean_data["Embarked"].fillna("U")
```
## Преобразование признаков
Очищенные данные выглядят так
![[Pasted image 20250407133605.png]]
### Преобразование категориальных данных в числовые
У нас есть категориальные столбцы пола и порта. Мы не можем просто сделать их 0, 1, 2 и тд, так как тогда могут пострадать модели из-за того, что получается есть зависимость - одно находится между другим. В таком случае нам нужно прямое кодирование
![[Pasted image 20250407133740.png]]
В пандасе есть для этого функция
```python
gender_dummi_columns = pd.get_dummies(clean_data["Sex"], prefix="Sex")
embarked_dummi_columns = pd.get_dummies(clean_data["Embarked"], prefix="Embarked")
clean_data = pd.concat([clean_data, gender_dummi_columns], axis=1)
clean_data = pd.concat([clean_data, embarked_dummi_columns], axis=1)
clean_data = clean_data.drop(["Sex", "Embarked"], axis=1)
```
Теперь данные выглядят так
![[Pasted image 20250407133854.png]]
А вот что делать с классом? Стоит ли применять к нему прямое кодирование. Есть такое правило
==Связан ли этот признак с результатом? То есть при увеличении класса повышается ли вероятность выжить?==
![[Pasted image 20250407134051.png]]
Нет, все наоборот. Поэтому выполняем кодирование
```python
class_dummi_columns = pd.get_dummies(clean_data["Pclass"], prefix="Class")
clean_data = pd.concat([clean_data, class_dummi_columns], axis=1)
clean_data = clean_data.drop("Pclass", axis=1)
```
### Биннинг
Столбец возраст можно разбить на числовой диапазон и также применить прямое кодирование? Зачем? Чтобы "развязать руки" модели. 
```python
age_bins =  [0, 10, 20, 30, 40, 50, 60, 70, 80]
cat_age = pd.cut(clean_data["Age"], age_bins)
clean_data["Categorized_age"] = cat_age
clean_data = clean_data.drop("Age", axis=1)
cagegorized_age_columns = pd.get_dummies(df['Categorized_age'], prefix='Categorized_age')
clean_data = pd.concat([clean_data, cagegorized_age_columns], axis=1)
clean_data = clean_data.drop(['Categorized_age'], axis=1)
```
Вот такая штука получается
![[Pasted image 20250407134401.png]]
Ну а дальше обучаем модели)
## Обучение моделей
Прежде чем обучать модели, разобьем данные на тренировочные, валидационные и тестовые
```python
labels = df["Survived"]
df = df.drop("Survived", axis=1)
features = df

features_train, features_validation_test, labels_train, labels_validation_test = train_test_split(
    features, labels, test_size=0.4, random_state=100)
features_validation, features_test, labels_validation, labels_test = train_test_split(
    features_validation_test, labels_validation_test, test_size=0.5, random_state=100)
```
А теперь обучаем все наши модели
```python
linear_reg = LogisticRegression()
linear_reg.fit(features_train, labels_train)

decision_tree_model = DecisionTreeClassifier()
decision_tree_model.fit(features_train, labels_train)

gaussian_model = GaussianNB()
gaussian_model.fit(features_train, labels_train)

svm_model = SVC()
svm_model.fit(features_train, labels_train)

random_forest_model = RandomForestClassifier()
random_forest_model.fit(features_train, labels_train)

gradient_boosting_model = GradientBoostingClassifier()
gradient_boosting_model.fit(features_train, labels_train)

adaboost_model = AdaBoostClassifier()
adaboost_model.fit(features_train, labels_train)
```
И посмотрим на их полноту
```python
print("Scores of the models")
print("Logistic regression:", linear_reg.score(features_validation, labels_validation))
print("Decision tree:", decision_tree_model.score(features_validation, labels_validation))
print("Naive Bayes:", gaussian_model.score(features_validation, labels_validation))
print("SVM:", svm_model.score(features_validation, labels_validation))
print("Random forest:", random_forest_model.score(features_validation, labels_validation))
print("Gradient boosting:", gradient_boosting_model.score(features_validation, labels_validation))
print("AdaBoost:", adaboost_model.score(features_validation, labels_validation))

Scores of the models
Logistic regression: 0.7696629213483146
Decision tree: 0.7752808988764045
Naive Bayes: 0.7471910112359551
SVM: 0.6797752808988764
Random forest: 0.7696629213483146
Gradient boosting: 0.8089887640449438
AdaBoost: 0.7359550561797753
```
Лучше всего себя показала модель градиентного бустинга. Проверим их F1метрики
```python
print("F1 metric")
linear_pred = linear_reg.predict(features_validation)
print("Logistic regression:", f1_score(labels_validation, linear_pred))

decision_tree_pred = decision_tree_model.predict(features_validation)
print("Decision tree:", f1_score(labels_validation, decision_tree_pred))

gaussian_model_pred = gaussian_model.predict(features_validation)
print("Naive Bayes:", f1_score(labels_validation, gaussian_model_pred))

svm_model_pred = svm_model.predict(features_validation)
print("SVM:", f1_score(labels_validation, svm_model_pred))

random_forest_model_pred = random_forest_model.predict(features_validation)
print("Random forest:", f1_score(labels_validation, random_forest_model_pred))

gradient_boosting_model_pred = gradient_boosting_model.predict(features_validation)
print("Gradient boosting:", f1_score(labels_validation, gradient_boosting_model_pred))

adaboost_model_pred = linear_reg.predict(features_validation)
print("AdaBoost:", f1_score(labels_validation, adaboost_model_pred))

F1 metric
Logistic regression: 0.6870229007633588
Decision tree: 0.696969696969697
Naive Bayes: 0.6808510638297872
SVM: 0.4
Random forest: 0.7050359712230215
Gradient boosting: 0.7384615384615385
AdaBoost: 0.6870229007633588
```
И опять же бустинг лучше всех. Проверяем, как она покажет себя на тестовом наборе
```python
gradient_boosting_model.score(features_test, labels_test)
0.8324022346368715
gradient_boosting_model_pred = gradient_boosting_model.predict(features_test)
print("Gradient boosting:", f1_score(labels_test, gradient_boosting_model_pred))
Gradient boosting: 0.8026315789473685
```
Отличный результат.
## Поиск по сетке
Мы использовали стандартные гиперпараметры моделей. Но мы можем проверить, может быть SVM покажет себя лучше на других гиперпараметрах? У нас есть параметр `C` и гамма. Мы могли бы вручную все прописывать, но в склерн есть встроенная функция, которая принимает словарь гиперпараметров и выбирает лучшую модель
```python
svm_parameters = {'kernel': ['rbf'],
                  'C': [0.01, 0.1, 1 , 10, 100],
                  'gamma': [0.01, 0.1, 1, 10, 100]
                }
svm = SVC()
svm_gs = GridSearchCV(estimator = svm,
                      param_grid = svm_parameters)
svm_gs.fit(features_train, labels_train)

svm_winner = svm_gs.best_estimator_
svm_winner

svm_winner.score(features_validation, labels_validation)
0.7191011235955056
```
Намного лучше чем было