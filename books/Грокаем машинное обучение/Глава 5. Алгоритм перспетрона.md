Тут про задачу классификации. 
## Идея классификации
Предположим мы на другой планете и мы хотим понимать, какое настроение у инопланетян. У них есть только 2 слова - "аак", "бип".
У нас есть такой набор данных
![[Pasted image 20250402115125.png]]
Получается, что если слов "аак" большем, чем "бип", то мы делаем предположение, что он радостный.
Присвоим словам баллы:
- "аак" - 1 балл
- "бип" - -1 балл
Если расположить это все на графике, то получится вот так
![[Pasted image 20250402115413.png]]
Все грустные расположены слева сверху, а радостные - справа снизу. Так получается, так как у грустных большее число слов "бип"
Их можно разделить прямой
![[Pasted image 20250402115509.png]]
Уравнение этой прямой такое:
```
кол-во акк - кол-во бип = 0
```
Либо если более формально
$$
x_{(аак)}-x_{(бип)}=0
$$
Вместе с этим уравнением мы получаем 2 важных понятия:
- **Положительная зона** - область на плоскости, в которой $x_{(аак)} - x_{(бип)}>0$. То есть настроение радостное
- **Отрицательная зона** - область на плоскости, в которой $x_{(аак)} - x_{(бип)}<0$. То есть настроение грустное
Это уже самый простой классификатор. На основе графика мы можем делать предположение о настроении
### Более сложная модель
![[Pasted image 20250402120118.png]]
Если расположить это на графике
![[Pasted image 20250402120146.png]]
Видно, что грустные те, кто мало говорит. Все предложения с 3 и менее словами - грустные, а с 4 и более - радостные. 
Добавим следующие баллы
- хрясь - 1 балл
- чпок - 1 балл
И такие правила. Для упрощения возьмем границу в 3.5 балла
- Если оценка составляет 3.5 балла и более, значит предложение радостное
- Если оценка составляет 3.5 балла и менее, значит предложение грустное
![[Pasted image 20250402120421.png]]
Важная деталь в этой модели - граничное значение 3.5 балла. Пороговые значения не используют. Использую смещение. Смещение - это значение, которое мы в итоге добавляем в баллам. Если сумма отрицательная - то предложение грустное, иначе радостное. Получаем такую прямую
$$
x_{хрясь} + x_{чпок} - 3.5=0
$$
Здесь положительная зона: $x_{хрясь}+x_{чпок}-3.5>=0$
Отрицательная зона: $x_{хрясь}+x_{чпок}-3.5<0$
Классификатор не обязан всегда давать верные ответ!
## Общий классификатор
Назовем наши слова 1 и 2, а кол-во появлений как $x_1$ и $x_2$. Тогда уравнение будет выглядеть так: $x_1+x_2-3.5=0$
Общая формула уравнения персептрона - $ax_1+bx_2+c=0$. $a$ - оценка слова 1, $b$ - оценка слова 2, $c$ - смещение
- Положительная зона: $ax_1+bx_2+c>=0$
- Отрицательная зона: $ax_1+bx_2+c<0$
## Ступенчатая функция и функция активации
Ступенчатая функция - функция, которая возвращает 1, если выходные данные неотрицательные, и 0, если отрицательные. Если входные данные равны $x$, то
- $step(x)=1, if \;x>=0$
- $step(x)=0, if\; x<0$
Теперь мы можем в более простой форме записать классификатор
$$
\hat{y}=step(ax_1+bx_2+c)
$$
В общем же виде персептронный классифактор записывается так. Предположим, что в языке n слов, которые обозначим 1,2,3,...n. Набор данных состоит из m предложений: $x^{(1)},x^{(2)},...x^{(m)}$ . Каждое предложение сопровождается меткой $y_i$, которая равна 1, если предложение радостное и 0, если грустное. 
Каждое предложение может рассматриваться как кортеж чисел: $x^{(i)}=(x_1^{(i)},x_2^{(i)},...)$, где $x_i^{(j)}$ - кол-во появлений слова $j$ в предложении $i$
Веса для каждого слова обозначаются как $w_i$, а смещение как $b$. В общем виде получается
$$
\hat{y}=step(w_1x_1^{(i)}+w_2x_2^{(i)}+...+w_nx_n^{(i)}+b)
$$
Перевожу: предсказание для какого-то предложения $i$ равно результату вызова ступенчатой функции, которая принимает сумму:
- $w_1x_1^{(i)}$ - значение веса для слова $x_1$. $i$ сверху говорит о том, что это для какого-то конкретного предложения
- плюс смещение
## Положительное и отрицательное смещение
А если инопланетянин молчит? Геометрическая разница между отрицательной и положительной зоной
![[Pasted image 20250402122705.png]]
Точка с координатами (0, 0) соответствует предложению без слов. В классификаторе с положительным смещением начало координат в положительной зоне, с отрицательным - в отрицательной
Примеры, когда смещение положительное и отрицательное
- **Положительное смещение**. Набор данных отзывов на озоне. Отрицательный или положительный отзыв зависит от кол-ва слов в отзыве. А если отзыв пустой? Обычно, если человек понравился продукт, он просто ставит 5 звезд и все. А вот если отрицательный, то слова много, так как много недовольства. Поэтому, скорее всего, пустой отзыв мы будет классифицировать как положительный. Поэтому смещение будет положительным
- **Отрицательное смещение**. Набор данных разговора с друзьями. Если друг молчит - скорее всего он обижен. А если говорит много, то нет. Поэтому если друг сказал 0 слов, то будем классифицировать это предложение как грустное. Поэтому смещение отрицательное
## Функция ошибки
![[Pasted image 20250402123523.png]]
### Функция ошибки 1. Количество ошибок
Мы можем просто посчитать, сколько ошибок допустил классификатор. Это неплохая функция, но хорошей ее не назовешь. Так как она не говорит, насколько велика эта ошибка
![[Pasted image 20250402123712.png]]
Эта ошибка плоха тем, что мы не можем использовать для нее градиентный спуск. Эта ошибка плоская. Мы не знаем, в какую сторону нам идти, чтобы ее уменьшить.
### Функция ошибки 2. Расстояние
![[Pasted image 20250402123834.png]]
Такая функция более эффективна:
- Если ошибки нет, она вернет 0
- Чем больше ошибка, тем больше будет результат функции
Это почти то, что будет применяться. Но не совсем. Расстояние вычисляется по формуле Пифагора, там есть корень, у корня плохая производная... В общем СЛОЖНА
### Функция ошибки 3. Оценка
Какая функция ошибки, которая нам нужна?
- Ошибка правильно классифицированной точке равна 0
- Ошибка неправильно классифицированной точки положительное число
	- Для неправильно классифицированной точки, близких к гарнце, значение ошибки не велико
	- Для неправильно классифицированной точки, далекой от границы, значение ошибки большое
- Простая формула
Рассмотрим такой классификатор: $\hat{y}=step(ax_{аак}+bx_{бип}+c)$ . Ошибка песептрона определяется так:
- Если предложение классифицировано правильно, то ошибка 0
- Если предложение классифицировано неправильно, то ошибка равно $| ax_{аак}+bx+{бип} +c |$
А вот так выглядит это в общем случае: $| w_1x_1+2_wx_2+...+w_nx_n+b |$
### Средняя ошибка персептрона
Чтобы вычислить ошибку для всего набора данных, берем среднее значение всех ошибок. Это называется средней ошибкой персептрона. Рассмотрим на примере
![[Pasted image 20250402124734.png]]
Сравним 2 классификатора
**Классификатор 1**
**Веса**:
- аак(a) = 1
- бип(b) = 2
**Смещение**: $c=-4$
**Прогноз**: $\hat{y}=step(1x_{аак}+2x_{бип}-4)$

**Классификатор 2**
**Веса**:
- аак(a) = -1
- бип(b) = 1
**Смещение**: $c=0$
**Прогноз**: $\hat{y}=step(-1x_{аак}+1x_{бип})$

Получаем такие результаты
![[Pasted image 20250402125121.png]]
А теперь сравним их по ошибке
![[Pasted image 20250402125646.png]]
Ошибка - это модуль он оценки. А средняя ошибка - ну, средняя ошибка). Получается, что классификатор 2 лучше, чем 1
## Алгоритм персептрона
Алгоритм похож на алгоритм линейной регрессии. Основные шаги
1. Начать со случайного перспептронного классификатора
2. Немного улучшить классификатор (повторить много раз)
3. Измерить ошибку, чтобы понять, когда остановиться
### Метод персептрона. Способ немного улучшить классификатор
**Случай 1**. Если точка классифицирована правильно, то оставить все как есть
**Случай 2**. Точка классифицирована неправильно, значит нужно немного поменять веса и смещение, чтобы ошибка стала меньше. Тут также используется скорость обучения $\eta$
**Улучшаем плохой классификатор**
**Веса**:
- аак = 1
- бип = 2
**Смещение**: $c=-4$
**Прогноз**: $\hat{y}=step(x_{аак}+2x_{бип}-4)$
**Предложение 1**: "Бип аак аак бип бип бип бип"
**Метка**: грустный (0)
А наш прогноз выдает веселый(1) со значением 8. Классификатор дал ему положительную оценку, а должен был отрицательную. Нам нужно уменьшить веса. Можем просто вычесть скорость обучения из весов и смещения. Но как будто слово "бип" более значимо, так как оно встречается чаще. Поэтому будем вычитать из веса скорость обучения помноженную на кол-во слова в предложении. 
Новый классификатор выглядит так:
- $a=w_a-\eta*2=1-0.01*2=0.98$
- $b=w_b-\eta*5=2-0.01*5=1.95$
- $c=c-\eta=-4.01$
Теперь ошибка для той же точки будет: $| 0.98*2 + 1.95*5 - 4.01 | = 7.7$. Ошибка стала меньше
Рассмотрим случай, когда веса нужно увеличить, так как классификатор закину точку в отрицательную зону, а должен был в положительную 
**Предложение 2**: "аак аак"
**Метка**: радостный(1)
**Прогноз**: $\hat{y}=step(1*2+0-4)=step(-2)=0$
Нам нужно немного увеличить веса, чтобы уменьшить ошибку.
- Слово "аак" встречается 2 раза, поэтому увеличим его так: $a=1+\eta*2=1+0.01*2=1.02$
- Слово "бип" встречается 0 раз, поэтому не трогаем его
- Смещение увеличиваем так: $c=-4+\eta=-4+0.01=-3.99$
Теперь ошибка равна $| 1.02*2+0-3.99 | = 1.95$. Стало лучше.
А теперь это можно записать в общем случае
**Входные данные**:
- перспетрон с весами $a$, $b$ и смещением $c$
- точка с координатами $x_{1}, x_2$  и метка $y$
- скорость обучения $\eta$
**Результат** - перспетрон с новыми весами $a$, $b$ и смещением $c$
**Процедура**:
- Прогноз для этой точки равен $\hat{y}=step(ax_1+bx_2+c)$
**Возврат**:
- Новые веса:
	- $a=a+\eta(y-\hat{y})x_1$
	- $b=b+\eta(y-\hat{y})x_2$
	- $c=c+\eta(y-\hat{y})$
Если точка классифицирована верна, то $y-\hat{y}=0$ и веса останутся неизменны. Если же $y-\hat{y}=-1$, то мы уменьшим вес, так как предсказанное значение имеет положительную ошибку. 
## Алгоритм перспетрона
**Входные данные**:
- набор размеченных точек
- кол-во эпох
- скорость обучения $\eta$
**Результат**: перспетронный классификатор
**Процедура**:
- Задать случайные значения весам и смещению
- Повторять многократно (n раз):
	- выбрать случайную точку данных
	- обновить веса и смещения с помощью алгоритма перспетрона
- **Возврат**: перспетронный классификатор