## Гиперпараметры
- `C` - настройка уровня регуляризации. Чем больше, тем сильнее веса у модели и значит, она с большей вероятностью переобучится 
- `penalty` - штрафная функция. Например `L1` или `L2` регуляризация. Работает не со всеми солверами
```  ================= ============================== ======================
       solver            penalty                        multinomial multiclass
       ================= ============================== ======================
       'lbfgs'           'l2', None                     yes
       'liblinear'       'l1', 'l2'                     no
       'newton-cg'       'l2', None                     yes
       'newton-cholesky' 'l2', None                     no
       'sag'             'l2', None                     yes
       'saga'            'elasticnet', 'l1', 'l2', None yes
       ================= ============================== ======================
```
- `solver` - алгоритм оптимизации
## Сводка
Логистическая регрессия - это алгоритм обучения с учителем, который используется для классификации. Работает, только если данные линейно-разделимы. 
## Сигмоида
Сигмоида, это функция, которая позволяет сжать непрерывные значения в диапазон между 0 и 1. Выход этой функции можно воспринимать как вероятность
$$
\sigma=\frac{1}{1+e^{-z}}
$$
Где $z$ - фактических выход от функции прямой с какими-то весами: $w_1*x_1+w_2*x_2+...+w_n*x_n+b$
## Функция потерь
$$
L(w,b)=\Sigma[-y^i*ln(\sigma(z^i))-(1-y^i)*ln(1-\sigma(z^i))]
$$
Эта формула посчитает ОБЩУЮ ошибку для всех точек. 
$$
\begin{equation*}L(\sigma(z), y;w,b) =\begin{cases} -ln(\sigma(z)~~~~~~~~~~~~~~~~~~if~y=1, \\ -ln(1-\sigma(z)))~~~~~~~~if~y=0\end{cases} \end{equation*}
$$
Либо это можно переписать так. 
![[newplot (1).png]]
Чем ближе к 1 значение сигмоиды при том условии, что метка 1, тем меньше потеря
Чем ближе к 1 значение сигмоиды при том условии, что метка 0, тем больше потеря
## Обучение в scikit-learn
### Обучение
```python
reg = LogisticRegression()
reg.fit(features_train, labels_train)
```
### Вычисление метрик
```python
print("score: ", reg.score(features_test, labels_test))
pred = reg.predict(features_test)
print("recall", sklearn.metrics.recall_score(labels_test, pred, average="micro"))
print("f1", sklearn.metrics.f1_score(labels_test, pred, average="micro"))
print("precision", sklearn.metrics.precision_score(labels_test, pred, average="micro"))
print("acu", sklearn.metrics.roc_auc_score(labels_test, reg.predict_proba(features_test), multi_class="ovr"))
```
Параметр `average` используется в случаях, когда у нас многклассовая классификация (много классов, но точка принадлежит только 1 классу)
- `micro` - считает метрику по всем экземплярам, а не для каждого класса отдельно. То есть считает метрику как сумму Tps и FPs значений по каждому классу. Используем, когда нужна общая метрика
- `macro` - считает метрику по каждому экземпляру, а потом усредняет. Используем, когда хотим оценить метрику по классам
- `weigthed` - тоже усредняет, но с учетом весов классов, то есть учитывает кол-во точек в каждом из классов. Используем, если классы несбалансированны
При расчете площади под кривой ROC, нужно указать параметр `multi_class="ovr"`, в случае многоклассовой классификации