[[Spark]]
Тут те же танцы с бубном. Но в целом то же самое, использует либо Maven Coords либо локально качаем jar пакет. И еще - Spark не может нормально работать с некоторыми типами клика
```python
import os

jar_path = os.path.expanduser("~/spark_drivers/clickhouse-jdbc-0.8.6-20250619.155740-1-all.jar")

spark = SparkSession.builder \
    .appName("PySpark ClickHouse Connection") \
    .config("spark.jars", jar_path) \
    .getOrCreate()
    
url = "jdbc:clickhouse://localhost:8123/default"
properties = {
    "user": "default",  
    "password": "", 
    "driver": "com.clickhouse.jdbc.ClickHouseDriver"
}

query = """
(SELECT
    date_sale,
    CAST(product_id AS Int32) AS product_id,
    CAST(quantity AS Int32) AS quantity,
    CAST(price AS Float32) AS price
 FROM sales_raw) AS sales_alias
"""
df = spark.read.jdbc(url=url, table=query, properties=properties)
df.show()
```
## Troubleshooting
- `ERROR Configuration: error parsing conf core-default.xml java.nio.file.NoSuchFileException: /Users/nikita/code/py_spark/venv/lib/python3.13/site-packages/pyspark/jars/hadoop-client-api-3.4.1.jar`  если он лезет в локальные файлы. Он там указывает версию апи хадупа. Так вот, нужно подбирать версию спарка и снапшота jdbc, чтоб все работало. 