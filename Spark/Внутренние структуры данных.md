[[Spark]]

## RDD
RDD - это распределенная коллеккция данных по класетеру. Неизменяемая. Обработка элементов этой коллекции происходит параллельно. RDD не имеет какой-то конкретной схемы - это просто данные
```python
from pyspark.sql import SparkSession


spark = SparkSession.builder.appName("FirstSparkApp").getOrCreate()
data = [1, 2, 3, 4, 5]

rdd = spark.sparkContext.parallelize(data)
rdd.reduce(lambda a, b: a + b)
```
## DataFrame
Это как в пандасе) Но только он может храниться и обрабатываться распределенно
```python
people = spark.createDataFrame([
    {"deptId": 1, "age": 40, "name": "Hyukjin Kwon", "gender": "M", "salary": 50},
    {"deptId": 1, "age": 50, "name": "Takuya Ueshin", "gender": "M", "salary": 100},
    {"deptId": 2, "age": 60, "name": "Xinrong Meng", "gender": "F", "salary": 150},
    {"deptId": 3, "age": 20, "name": "Haejoon Lee", "gender": "M", "salary": 200}
])
people.show()
```
```
+---+------+------+-------------+------+
|age|deptId|gender|         name|salary|
+---+------+------+-------------+------+
| 40|     1|     M| Hyukjin Kwon|    50|
| 50|     1|     M|Takuya Ueshin|   100|
| 60|     2|     F| Xinrong Meng|   150|
| 20|     3|     M|  Haejoon Lee|   200|
+---+------+------+-------------+------+
```
## Dataset
Эта штука доступно только в Spark на Java или Scala, в питоне его нет. Основные отличия в том, что типы определяются не в рантайме, а на этапе компиляции. В общем более оптимизирован