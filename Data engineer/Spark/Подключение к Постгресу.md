[[Spark]]
Тут нужно танцевать с бубном. Для того, чтобы можно было подключиться к БД нужно скачать драйвер JDBC для этой СУБД. Для постгреса - https://jdbc.postgresql.org/download/

Потом нужно прописать ПОЛНЫЙ путь до файла, без всяких `~`, так как спарк не понимает, что это

И, оч важно, нужно этот путь поставить как значение ключу конфига `spark.jars`

Либо можно не качать локально, а использовать Maven Coord
Вот 2 варианта:
**Локально**
```python
from pyspark.sql import SparkSession
import os

jar_path = os.path.expanduser("~/spark_drivers/postgresql-42.7.7.jar")

spark = SparkSession.builder \
    .appName("Postgres PySpark") \
    .config("spark.jars", jar_path)\
    .master("local[*]") \
    .getOrCreate()
```
**И с Maven:**
```python
spark2 = SparkSession.builder \
    .appName("Postgres PySpark") \
    .config("spark.jars.packages", "org.postgresql:postgresql:42.7.7")\
    .master("local[*]") \
    .getOrCreate()
```
Дальше все одинаково:
```python
db_url = "jdbc:postgresql://localhost:5432/db"
properties = {
    "user": "admin",
    "password": "1234",
    "driver": "org.postgresql.Driver"
}
df = spark2.read.jdbc(url=db_url, table="employee", properties=properties)
```
Теперь в df лежит таблица employee. Может работать с ней через SQL API или DataFrame API
```python
df.createOrReplaceTempView("tbl")
spark.sql("select * from tbl where salary > 1000 and supervisor is not null order by name").show()

+-----+------+----------+------+
|empid|  name|supervisor|salary|
+-----+------+----------+------+
|    2|   Dan|         3|  2000|
|    4|Thomas|         3|  4000|
+-----+------+----------+------+
```
```python
df.select(["*"]).filter((df.salary > 1000) & (df.supervisor.isNotNull())).sort(df.name).show()

```

## Troubleshooting
Если не может найтись драйвер постгреса, то:
- Проверить путь. При ините конфига будут логи, в которых будет ERROR с текстом, что не можем найти файл по такому пути. 
- Проверить энвы спарка в spark UI. Драйвер должен быть либо в spark.jars, либо в spark.jars.packages
- Попробовать пропихнуть файл насильно в jvm: `export PYSPARK_SUBMIT_ARGS="--jars /Users/nikita/spark_drivers/postgresql-42.7.7.jar pyspark-shell"`