Здесь описано, какие этапы подготовки данных нужно выполнить, чтобы получить от моедли максимальный профит. Все выполняется по порядку.
## Отсутствующие данные 
Чтобы проверить кол-во отсутствующих данных по каждому столбцу в DataFrame:
```python
df.isnull().sum()
```
### Удаление
Мы можем удалить строки или столбцы по определенным условиям, где есть пропуск.
**Удаление всех строк, где есть хоть один nan**:
```python
df.dropna(axis=0)
```
**Удаление всех колонок, где есть хоть один nan**
```python
df.dropna(axis=1)
```
**Удаление строк/столбцов, где все значения nan**
```python
df.dropna(how="all", axis=1/0)
```
**Удаление строк/столбцов, где менее n реальных значений**
```python
df.dropna(thresh=n, axis=1/0)
```
**Удаление строк, в которых nan содержится только в заданных столбцах**
```python
df.dropna(subset=["C"])
```
Тут удалятся те строки, в которых в колонке `C` стоит `nan`
### Подстановка пропущенных значений
Не всегда удаление пропусков является хорошей идеей, так как тогда мы потеряем очень много данных. Вместо этого эти пропуски можно заполнять:
- Средним значением этого признака
- Модой этого признака (для категориальных переменных)
- Медианой этого признака
Сделать этого либо через метод DataFrame `fillna`
```python
df.fillna(df.mean())
```
Либо через класс в sklearn:
```python
from sklearn.impute import SimpleImputer
imt = SimpleImputer(missing_values=np.nan, strategy="most_frequent/median/mean")
imt.fit(df.values)
imt.transform(df.values)
```
Тут возвращается матрица, но ее можно переделать обратно в DataFrame
Еще одним способ заполнения пропусков является использования алгоритма kNN:
```python
from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=2, weights="uniform")
imputer.fit_transform(df.values)
```
Также возвращается двумерный массив. Но тут надо правильно указать параметр `n_neighbors`
## Категориальные данные
Категориальные данные можно разбить на две группы:
- порядковые 
- номинальные
Порядковые признаки можно упорядочить, например, размер одежды. Номинальные сортировать не получится, например, цвет одежды. 
### Категориальные порядковые признаки
Чтобы модель правильно распознавала порядковые категориальные признаки, нужно превратить их в число. Например, у нас признак Размер одежды: XL, M, L. Нужно присвоить каждому из них число:
```python
size_mapping = {"XL": 3, "L": 2, "M": 1}
df["size"] = df["size"].map(size_mapping)
```
Теперь колонка `size` будет содержать числовые признаки. 
### Кодирование меток класса
Метки класса не являются порядковыми, поэтому неважно какое число будет стоять за каждым классом, поэтому можем начинать кодировать метки просто с 0
Тут уже есть готовый класс в sklearn:
```python
from sklearn.preprocessing import LabelEncoder
class_le = LabelEncoder()
y = class_le.fit_transform(df["label"].values)
```
Дальше просто заменяем этим столбцом старый. Также мы можем "получить значения обратно":
```python
class_le.inverse_transform(y)
```
Либо можем это сделать руками:
```python
class_mapping = {label: idx for idx, label in enumerate(np.unique(df["classlabel"]))}
df["classlabel"] = df["classlabel"].map(class_mapping)
```
### Позиционное кодирование номинальных признаков
Мы не можем присвоить номинальным признакам, например, цвету одежды какой-то номер, так как тогда модели классификации будут считать, что один цвет больше другого, что приведет к неправильному обучению. Поэтому тут нужно использовать другой подход - one hot encoding.  
One hot encoding создает новый фиктивный признак для каждого уникального значения в столбце. Рассмотрим пример с цветом. Предположим у нас три цвета: green, blue, red. И мы кодируем каждый признак как набор других, например 100(green 1, blue 0, red 0). В sklearn есть для этого класс:
```python
X = df.loc[:, ["color", "size", "price"]].values
color_ohe = OneHotEncoder()
color_ohe.fit_transform(X[:, 0].reshape(-1, 1)).toarray()
array([[0., 1., 0.],
       [0., 0., 1.],
       [1., 0., 0.]])
```
Первая строка соответствует какому-то признаку, вторая тоже и тд.
Если необходимо выборочно преобразовать столбцы, то можно использовать класс `ColumnTransformer`:
```python
from sklearn.compose import ColumnTransformer

X = df[["color", "size", "price"]].values
c_transf = ColumnTransformer([
    ("onehot", OneHotEncoder(), [0]),
    ("nothing", "passthrough", [1, 2])
])
c_transf.fit_transform(X).astype(np.float64)
array([[ 0. ,  1. ,  0. ,  1. , 10.1],
       [ 0. ,  0. ,  1. ,  2. , 13.5],
       [ 1. ,  0. ,  0. ,  3. , 15.3]])
```
Тут мы говорим, что нужно применить метод `onehot` для столбца с индексом 0, а столбцы с индексами 1 и 2 оставить как есть.
Еще один способ создания фиктивных переменных - это функция `get_dummies`
```python
pd.get_dummies(df[["price", "color", "size"]])
   price  color_blue  color_green  color_red  size_L  size_M  size_XL
0   10.1       False         True      False   False    True    False
1   13.5       False        False       True    True   False    False
2   15.3        True        False      False   False   False     True
```
Эта функция будет преобразовывать только строковые столбцы.
При прямом кодировании может возникнуть проблема, сильная корреляция. Чтобы ее уменьшить ее, достаточно удалить первый столбец у каждого признака. 
## Разбиение набора
### Hold oud
Для разбиения используется функция из sklearn `train_test_split`. Она принимает следующие важные параметры:
- `test_size` - процент тестовой выборки от общего набора
- `stratify` - массив по чему стратифицировать
Стратификация по массиву меток позволит гарантировать, что соотношение меток класса будет таким же как и в исходном наборе. Стратификацию стоит применять практически всегда, когда стоит задача классификации.
Также если выборка достаточно большая, то стоит добавить еще один набор - валидационный - 60% для тренировки, 20% для валидации и 20% для теста. Это соотношение может меняться в зависимости от размера датасета. Если он ну очень больщой, то можно делать 80/10/10. На валидационном наборе мы выбираем лучшую модель. ТЕСТОВЫЙ В САМОМ КОНЦЕ!!!
Такое разделение хорошо на больших наборах данных, так как в случае большого кол-ва данных наборы и так будут разные, плюс такой подход задействует мало вычислительных ресурсов
### K-fold
В таком подходе мы делим ВСЕ данные на k частей, фолдов
![[Pasted image 20250412133129.png]]
Далее происходит k-итераций. На каждой итерации модель обучается на i-ом наборе. Оценка модели происходит по среднему значению оценок от каждой оценки, либо, если мы делили данные на тестовые и тренировочные, то на тестовом наборе. Такой подход лучше применять на малом кол-ве данных, либо если позволяют вычислительные мощности выполнять k итераций параллельно.
```python
scores = cross_val_score(estimator=log_regression, X=features_train, y=labels_train)
```

## В целом
==Эти 2 подхода не взаимоисключающие!!!==. Даже, если мы используем кросс валидацию есть смысл дробить данные на тестовую и обучающую часть, если данных мало, и на 3 части, если много.
## Приведение признаков к одному масштабу
==ОБУЧАТЬ ТОЛЬКО НА ТРЕЙН НАБОР==
Алгоритмы Дерево решений или рандомный форест не восприимчивы к масштабу признаков. Но вот kNN или логистическая регрессия довольно восприимчивы, так как функция ошибки зависит от расстояния. А если у нас один признак в пределах от 1 до 10, а другой от 1 до 1000000, то алгоритм в основном будет занят оптимизацией бОльшего признака. Но выход есть!
Мы можем привести признаки к одному масштабу, используя минимакс масштабирования или стандартизацию. 
Минимакс масштабирование сводит все значения в какой-то диапазон. А стандартизация преобразует данные в вид, когда у них нулевое средние и единичная дисперсия. Стандартизация в основном более часто применяется, так как хорошо стакается с градиентным спуском и менее чувствительная к выбросам
```python
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
mms = MinMaxScaler()
features_train_norm = mms.fit_transform(features_train)

stdsc = StandardScaler()
features_train_std = stdsc.fit_transform(features_train)

robust = RobustScaler()
features_train_rob = robust.fit_transform(features_train)
```
Робуст полезен в случаях, когда данных мало и в них много выбросов. Также он хорош, если модель склонна к переобучению. 
## Выбор релевантных признаков (feature selection)
==ОБУЧАТЬ ТОЛЬКО НА ТРЕЙНЕ==
На этом этапе мы выбираем те признаки, которые важны в контексте задачи. При выборе релевантных признаков мы именно выбираем из контекста задачи. Это важно. Также перед этим нужно стандартизировать данные.
Выбрать релевантные признаки может помочь случайный лес
```python
from sklearn.ensemble import RandomForestClassifier

feat_labels = df.columns[1:]
forest = RandomForestClassifier(n_estimators=500, random_state=1)
forest.fit(features_train, labels_train)
importances = forest.feature_importances_
indicies = np.argsort(importances)[::-1]
importance = pd.DataFrame({
    "feature_name": [feat_labels[i] for i in indicies],
    "feature_importance": [importances[i] for i in indicies]
})
px.bar(importance, x="feature_name", y="feature_importance")
```
![[newplot (4).png]]
Либо можно воспользоваться классов `SelectFromModel`:
```python
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=500)
rf.fit(X_train, y_train)

sfm = SelectFromModel(rf, threshold=0.1, prefit=True)
X_selected = sfm.transform(X_train)
```
На выходе мы получим матрицу релевантных признаков. Параметр `threshold=0.01` говорит о том, что нужно выбрать те признаки, у которых значимость не менее 1%. `prefit=True` говорит о том, что модель уже обучена
## Анализ основных компонент
==ОБУЧАТЬ ТОЛЬКО НА ТРЕЙНЕ==
После того, как мы выбрали важные признаки в контексте задачи, нужно избавиться от избыточности и коллениарности
### PCA
Этот метод уменьшает размерность линейно. 
```python
from sklearn.decomposition import PCA
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
features_train_pca = pca.fit_transform(features_train_std)
```
Это выберет 2 главные фичи в наборе. Либо можно передать `None` и тогда он отсортирует их по вкладу:
```python
pca = PCA()
pca.fit(features_train_std)
explained = pca.explained_variance_ratio_
com = explained.cumsum()
```
### LDA
Этот метод уменьшает размерность линейно. 
Также есть еще один способ уменьшения размерности. Этот алгоритм с учителем
```python
lda = LDA(n_components=2)
features_train_lda = lda.fit_transform(features_train_std, labels_train)
features_test_lda = lda.transform(features_test_std)
```
### Нелинейной уменьшение размерности, t-SNE
Нелинейное уменьшение размерности хорошо работает с данными, которые нельзя разделить линейно. Но из-за того, что их сложно настроить большинство все равно выбирают PCA или LDA. 
Метод t-SNE используется для визуализации, чтобы сжать n-мерное пространство до двухмерного, чтобы можно было визуализировать данные
```python
from sklearn.datasets import load_digits
from sklearn.manifold import TSNE
import plotly.express as px

digits = load_digits()
labels_digits = digits.target
features_digits = digits.data
print(features_digits.shape)  # (1797, 64)
tsne = TSNE(n_components=2, init="pca", random_state=123)
features_tsne = tsne.fit_transform(features_digits)
digits_df = pd.DataFrame({
    "x1": features_tsne[:, 0],
    "x2": features_tsne[:, 1],
    "y": labels_digits,
})
px.scatter(digits_df, x="x1", y="x2", color="y")
```
![[newplot (5).png]]
Мы сжали 64-мерное пространство до двухмерного и теперь мы можем визуализировать данные
