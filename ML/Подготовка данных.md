## Отсутствующие данные 
Чтобы проверить кол-во отсутствующих данных по каждому столбцу в DataFrame:
```python
df.isnull().sum()
```
### Удаление
Мы можем удалить строки или столбцы по определенным условиям, где есть пропуск.
**Удаление всех строк, где есть хоть один nan**:
```python
df.dropna(axis=0)
```
**Удаление всех колонок, где есть хоть один nan**
```python
df.dropna(axis=1)
```
**Удаление строк/столбцов, где все значения nan**
```python
df.dropna(how="all", axis=1/0)
```
**Удаление строк/столбцов, где менее n реальных значений**
```python
df.dropna(thresh=n, axis=1/0)
```
**Удаление строк, в которых nan содержится только в заданных столбцах**
```python
df.dropna(subset=["C"])
```
Тут удалятся те строки, в которых в колонке `C` стоит `nan`
### Подстановка пропущенных значений
Не всегда удаление пропусков является хорошей идеей, так как тогда мы потеряем очень много данных. Вместо этого эти пропуски можно заполнять:
- Средним значением этого признака
- Модой этого признака (для категориальных переменных)
- Медианой этого признака
Сделать этого либо через метод DataFrame `fillna`
```python
df.fillna(df.mean())
```
Либо через класс в sklearn:
```python
from sklearn.impute import SimpleImputer
imt = SimpleImputer(missing_values=np.nan, strategy="most_frequent/median/mean")
imt.fit(df.values)
imt.transform(df.values)
```
Тут возвращается матрица, но ее можно переделать обратно в DataFrame
Еще одним способ заполнения пропусков является использования алгоритма kNN:
```python
from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=2, weights="uniform")
imputer.fit_transform(df.values)
```
Также возвращается двумерный массив. Но тут надо правильно указать параметр `n_neighbors`
## Категориальные данные
Категориальные данные можно разбить на две группы:
- порядковые 
- номинальные
Порядковые признаки можно упорядочить, например, размер одежды. Номинальные сортировать не получится, например, цвет одежды. 
### Категориальные порядковые признаки
Чтобы модель правильно распознавала порядковые категориальные признаки, нужно превратить их в число. Например, у нас признак Размер одежды: XL, M, L. Нужно присвоить каждому из них число:
```python
size_mapping = {"XL": 3, "L": 2, "M": 1}
df["size"] = df["size"].map(size_mapping)
```
Теперь колонка `size` будет содержать числовые признаки. 
### Кодирование меток класса
Метки класса не являются порядковыми, поэтому неважно какое число будет стоять за каждым классом, поэтому можем начинать кодировать метки просто с 0
Тут уже есть готовый класс в sklearn:
```python
from sklearn.preprocessing import LabelEncoder
class_le = LabelEncoder()
y = class_le.fit_transform(df["label"].values)
```
Дальше просто заменяем этим столбцом старый. Также мы можем "получить значения обратно":
```python
class_le.inverse_transform(y)
```
Либо можем это сделать руками:
```python
class_mapping = {label: idx for idx, label in enumerate(np.unique(df["classlabel"]))}
df["classlabel"] = df["classlabel"].map(class_mapping)
```
### Позиционное кодирование номинальных признаков
Мы не можем присвоить номинальным признакам, например, цвету одежды какой-то номер, так как тогда модели классификации будут считать, что один цвет больше другого, что приведет к неправильному обучению. Поэтому тут нужно использовать другой подход - one hot encoding.  
One hot encoding создает новый фиктивный признак для каждого уникального значения в столбце. Рассмотрим пример с цветом. Предположим у нас три цвета: green, blue, red. И мы кодируем каждый признак как набор других, например 100(green 1, blue 0, red 0). В sklearn есть для этого класс:
```python
X = df.loc[:, ["color", "size", "price"]].values
color_ohe = OneHotEncoder()
color_ohe.fit_transform(X[:, 0].reshape(-1, 1)).toarray()
array([[0., 1., 0.],
       [0., 0., 1.],
       [1., 0., 0.]])
```
Первая строка соответствует какому-то признаку, вторая тоже и тд.
Если необходимо выборочно преобразовать столбцы, то можно использовать класс `ColumnTransformer`:
```python
from sklearn.compose import ColumnTransformer

X = df[["color", "size", "price"]].values
c_transf = ColumnTransformer([
    ("onehot", OneHotEncoder(), [0]),
    ("nothing", "passthrough", [1, 2])
])
c_transf.fit_transform(X).astype(np.float64)
array([[ 0. ,  1. ,  0. ,  1. , 10.1],
       [ 0. ,  0. ,  1. ,  2. , 13.5],
       [ 1. ,  0. ,  0. ,  3. , 15.3]])
```
Тут мы говорим, что нужно применить метод `onehot` для столбца с индексом 0, а столбцы с индексами 1 и 2 оставить как есть.
Еще один способ создания фиктивных переменных - это функция `get_dummies`
```python
pd.get_dummies(df[["price", "color", "size"]])
   price  color_blue  color_green  color_red  size_L  size_M  size_XL
0   10.1       False         True      False   False    True    False
1   13.5       False        False       True    True   False    False
2   15.3        True        False      False   False   False     True
```
Эта функция будет преобразовывать только строковые столбцы.
При прямом кодировании может возникнуть проблема, сильная корреляция. Чтобы ее уменьшить ее, достаточно удалить первый столбец у каждого признака. 
## Разбиение набора
Для разбиения используется функция из sklearn `train_test_split`. Она принимает следующие важные параметры:
- `test_size` - процент тестовой выборки от общего набора
- `stratify` - массив по чему стратифицировать
Стратификация по массиву меток позволит гарантировать, что соотношение меток класса будет таким же как и в исходном наборе. Стратификацию стоит применять практически всегда, когда стоит задача классификации.
Также если выборка достаточно большая, то стоит добавить еще один набор - валидационный - 60% для тренировки, 20% для валидации и 20% для теста. Это соотношение может меняться в зависимости от размера датасета. Если он ну очень больщой, то можно делать 80/10/10. На валидационном наборе мы выбираем лучшую модель. ТЕСТОВЫЙ В САМОМ КОНЦЕ!!!
## Приведение признаков к одному масштабу
Алгоритмы Дерево решений или рандомный форест не восприимчивы к масштабу признаков. Но вот kNN или логистическая регрессия довольно восприимчивы, так как функция ошибки зависит от расстояния. А если у нас один признак в пределах от 1 до 10, а другой от 1 до 1000000, то алгоритм в основном будет занят оптимизацией бОльшего признака. Но выход есть!
Мы можем привести признаки к одному масштабу, используя минимакс масштабирования или стандартизацию. 
Минимакс масштабирование сводит все значения в какой-то диапазон. А стандартизация преобразует данные в вид, когда у них нулевое средние и единичная дисперсия. Стандартизация в основном более часто применяется, так как хорошо стакается с градиентным спуском и менее чувствительная к выбросам
```python
from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler
mms = MinMaxScaler()
features_train_norm = mms.fit_transform(features_train)
features_test_norm = mms.fit_transform(features_test)

stdsc = StandardScaler()
features_train_std = stdsc.fit_transform(features_train)
features_test_std = stdsc.transform(features_test)

robust = RobustScaler()
features_train_rob = robust.fit_transform(features_train)
features_test_rob = robust.transform(features_test)
```
Робуст полезен в случаях, когда данных мало и в них много выбросов. Также он хорош, если модель склонна к переобучению
## Выбор релевантных признаков
Выбрать релевантные признаки может помочь случайный лес
```python
from sklearn.ensemble import RandomForestClassifier

feat_labels = df.columns[1:]
forest = RandomForestClassifier(n_estimators=500, random_state=1)
forest.fit(features_train, labels_train)
importances = forest.feature_importances_
indicies = np.argsort(importances)[::-1]
importance = pd.DataFrame({
    "feature_name": [feat_labels[i] for i in indicies],
    "feature_importance": [importances[i] for i in indicies]
})
px.bar(importance, x="feature_name", y="feature_importance")
```
![[newplot (4).png]]
